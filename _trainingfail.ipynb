{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d357d180",
   "metadata": {},
   "source": [
    "# MABe – Reservoir Computing Training\n",
    "\n",
    "This notebook loads the preprocessed dataset and trains Reservoir Computing models.\n",
    "It uses the data prepared by `02_dataset_processing_and_scaling.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc5f4389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: reservoirpy in c:\\users\\flore\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.4.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\flore\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.7.2)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\flore\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (3.10.8)\n",
      "Requirement already satisfied: joblib>=0.14.1 in c:\\users\\flore\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from reservoirpy) (1.5.3)\n",
      "Requirement already satisfied: numpy>=1.21.1 in c:\\users\\flore\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from reservoirpy) (2.2.6)\n",
      "Requirement already satisfied: scipy>=1.4.1 in c:\\users\\flore\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from reservoirpy) (1.15.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\flore\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\flore\\appdata\\roaming\\python\\python310\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\flore\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\flore\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: pyparsing>=3 in c:\\users\\flore\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (3.3.1)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\flore\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (12.1.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\flore\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\flore\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (4.61.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\flore\\appdata\\roaming\\python\\python310\\site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\flore\\appdata\\roaming\\python\\python310\\site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install reservoirpy scikit-learn matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3d0759",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from reservoirpy.nodes import Reservoir\n",
    "from sklearn.model_selection import GroupShuffleSplit, train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (classification_report, confusion_matrix, accuracy_score, \n",
    "                           precision_recall_curve, auc, f1_score, precision_score, recall_score)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import pickle\n",
    "import json\n",
    "import pandas as pd\n",
    "from typing import Tuple, Dict, Any\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "def build_reservoir(units: int = 300, sr: float = 0.9, lr: float = 0.3, \n",
    "                   input_scaling: float = 0.5, seed: int = 42) -> Reservoir:\n",
    "    \"\"\"Build an Echo State Network reservoir with fixed parameters.\"\"\"\n",
    "    return Reservoir(\n",
    "        units=units,\n",
    "        sr=sr,     # spectral radius\n",
    "        lr=lr,     # leaking rate\n",
    "        input_scaling=input_scaling,\n",
    "        seed=seed\n",
    "    )\n",
    "\n",
    "def reservoir_transform(reservoir: Reservoir, X_batch: np.ndarray, pooling: str = \"last\") -> np.ndarray:\n",
    "    \"\"\"Transform windowed time series through reservoir and pool states.\n",
    "    \n",
    "    Args:\n",
    "        reservoir: Trained reservoir\n",
    "        X_batch: (n_windows, window_size, n_features)\n",
    "        pooling: \"last\" or \"mean\"\n",
    "    \n",
    "    Returns:\n",
    "        Z: (n_windows, reservoir_units) pooled features\n",
    "    \"\"\"\n",
    "    n_windows = X_batch.shape[0]\n",
    "    reservoir_units = reservoir.units\n",
    "    Z = np.zeros((n_windows, reservoir_units), dtype=np.float32)\n",
    "    \n",
    "    for i in range(n_windows):\n",
    "        states = reservoir.run(X_batch[i])\n",
    "        if pooling == \"last\":\n",
    "            Z[i] = states[-1]  # Last state\n",
    "        elif pooling == \"mean\":\n",
    "            Z[i] = states.mean(axis=0)  # Mean across time\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown pooling: {pooling}\")\n",
    "        reservoir.reset()\n",
    "    \n",
    "    return Z\n",
    "\n",
    "def create_groups_from_metadata(file_info: list, y_all: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Create groups array for GroupShuffleSplit based on video files.\"\"\"\n",
    "    groups = np.zeros(len(y_all), dtype=object)\n",
    "    start_idx = 0\n",
    "    \n",
    "    for info in file_info:\n",
    "        n_windows = info['n_windows']\n",
    "        video_id = info['file'].strip()  # Remove any whitespace\n",
    "        groups[start_idx:start_idx + n_windows] = video_id\n",
    "        start_idx += n_windows\n",
    "    \n",
    "    return groups\n",
    "\n",
    "def prepare_binary_labels(y_all: np.ndarray, none_id: int) -> np.ndarray:\n",
    "    \"\"\"Convert multi-class labels to binary: action (1) vs none (0).\"\"\"\n",
    "    return (y_all != none_id).astype(int)\n",
    "\n",
    "def prepare_action_labels(y_all: np.ndarray, none_id: int, action_classes: list, class_to_id: dict) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Extract action windows and their labels for stage B training/evaluation.\"\"\"\n",
    "    action_mask = y_all != none_id\n",
    "    y_action = y_all[action_mask]\n",
    "    \n",
    "    # Convert to 0-3 range for 4 action classes\n",
    "    action_id_to_local = {class_to_id[cls]: i for i, cls in enumerate(action_classes)}\n",
    "    y_action_local = np.array([action_id_to_local[label] for label in y_action])\n",
    "    \n",
    "    return action_mask, y_action_local\n",
    "\n",
    "def train_and_eval_stageA(features: np.ndarray, y_binary: np.ndarray, groups: np.ndarray, \n",
    "                         feature_name: str) -> Dict[str, Any]:\n",
    "    \"\"\"Train and evaluate Stage A: Action vs None detection.\"\"\"\n",
    "    \n",
    "    # Group-based train/val split (80/20)\n",
    "    gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "    train_idx, val_idx = next(gss.split(features, y_binary, groups))\n",
    "    \n",
    "    X_train, X_val = features[train_idx], features[val_idx]\n",
    "    y_train, y_val = y_binary[train_idx], y_binary[val_idx]\n",
    "    \n",
    "    # Train logistic regression\n",
    "    clf = LogisticRegression(\n",
    "        solver=\"saga\", \n",
    "        max_iter=5000, \n",
    "        class_weight=\"balanced\",\n",
    "        random_state=42\n",
    "    )\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    # Get probabilities for threshold tuning\n",
    "    y_val_prob = clf.predict_proba(X_val)[:, 1]\n",
    "    \n",
    "    # Find best threshold using F-beta score (beta=2 favors recall)\n",
    "    thresholds = np.linspace(0.01, 0.99, 99)\n",
    "    fbeta_scores = []\n",
    "    \n",
    "    for thresh in thresholds:\n",
    "        y_pred_thresh = (y_val_prob >= thresh).astype(int)\n",
    "        # F-beta with beta=2 (favors recall)\n",
    "        beta = 2\n",
    "        prec = precision_score(y_val, y_pred_thresh, zero_division=0)\n",
    "        rec = recall_score(y_val, y_pred_thresh, zero_division=0)\n",
    "        if prec + rec > 0:\n",
    "            fbeta = (1 + beta**2) * (prec * rec) / (beta**2 * prec + rec)\n",
    "        else:\n",
    "            fbeta = 0\n",
    "        fbeta_scores.append(fbeta)\n",
    "    \n",
    "    best_idx = np.argmax(fbeta_scores)\n",
    "    best_threshold = thresholds[best_idx]\n",
    "    \n",
    "    # Evaluate at best threshold\n",
    "    y_val_pred = (y_val_prob >= best_threshold).astype(int)\n",
    "    \n",
    "    # Calculate PR-AUC\n",
    "    precision_curve, recall_curve, _ = precision_recall_curve(y_val, y_val_prob)\n",
    "    pr_auc = auc(recall_curve, precision_curve)\n",
    "    \n",
    "    results = {\n",
    "        'feature_name': feature_name,\n",
    "        'classifier': clf,\n",
    "        'best_threshold': best_threshold,\n",
    "        'pr_auc': pr_auc,\n",
    "        'precision': precision_score(y_val, y_val_pred, zero_division=0),\n",
    "        'recall': recall_score(y_val, y_val_pred, zero_division=0),\n",
    "        'f1': f1_score(y_val, y_val_pred, zero_division=0),\n",
    "        'fbeta': fbeta_scores[best_idx],\n",
    "        'confusion_matrix': confusion_matrix(y_val, y_val_pred),\n",
    "        'y_val_prob': y_val_prob,\n",
    "        'y_val_true': y_val,\n",
    "        'threshold_sweep': {\n",
    "            'thresholds': thresholds,\n",
    "            'fbeta_scores': fbeta_scores\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "def train_and_eval_stageB(features: np.ndarray, y_action: np.ndarray, groups: np.ndarray,\n",
    "                         action_mask: np.ndarray, feature_name: str, action_classes: list) -> Dict[str, Any]:\n",
    "    \"\"\"Train and evaluate Stage B: Multi-class action classification.\"\"\"\n",
    "    \n",
    "    # Only use features from action windows\n",
    "    features_action = features[action_mask]\n",
    "    \n",
    "    # Group-based train/val split on action windows only\n",
    "    action_groups = groups[action_mask]\n",
    "    gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "    train_idx, val_idx = next(gss.split(features_action, y_action, action_groups))\n",
    "    \n",
    "    X_train, X_val = features_action[train_idx], features_action[val_idx]\n",
    "    y_train, y_val = y_action[train_idx], y_action[val_idx]\n",
    "    \n",
    "    # Train logistic regression for multi-class\n",
    "    clf = LogisticRegression(\n",
    "        solver=\"saga\",\n",
    "        max_iter=5000,\n",
    "        class_weight=\"balanced\",\n",
    "        multi_class=\"multinomial\",\n",
    "        random_state=42\n",
    "    )\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict\n",
    "    y_val_pred = clf.predict(X_val)\n",
    "    \n",
    "    results = {\n",
    "        'feature_name': feature_name,\n",
    "        'classifier': clf,\n",
    "        'classification_report': classification_report(y_val, y_val_pred, \n",
    "                                                     target_names=action_classes, \n",
    "                                                     output_dict=True, zero_division=0),\n",
    "        'confusion_matrix': confusion_matrix(y_val, y_val_pred),\n",
    "        'macro_f1': f1_score(y_val, y_val_pred, average='macro', zero_division=0),\n",
    "        'y_val_pred': y_val_pred,\n",
    "        'y_val_true': y_val\n",
    "    }\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4d4a2b",
   "metadata": {},
   "source": [
    "## Load processed dataset\n",
    "\n",
    "Load the windowed data created by the preprocessing notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "96535338",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded:\n",
      "  X shape: (16395, 200, 10)\n",
      "  y shape: (16395,)\n",
      "  Classes: ['chase', 'avoid', 'attack', 'chaseattack', 'none']\n",
      "  Class distribution:\n",
      "    chase: 22 (0.001)\n",
      "    avoid: 52 (0.003)\n",
      "    attack: 111 (0.007)\n",
      "    chaseattack: 27 (0.002)\n",
      "    none: 16,183 (0.987)\n",
      "  Files processed: 10\n",
      "  Total windows: 16395\n",
      "  Features per timestep: 10\n",
      "  Unique video groups: 10\n",
      "\n",
      "Two-stage setup:\n",
      "  Stage A (binary): 212/16395 action windows (0.013)\n",
      "  Stage B (4-class): 212 action windows for classification\n",
      "  Action classes: ['chase', 'avoid', 'attack', 'chaseattack']\n"
     ]
    }
   ],
   "source": [
    "# Load processed dataset\n",
    "PROCESSED_ROOT = Path(\"data/data_processed\")\n",
    "\n",
    "X_all = np.load(PROCESSED_ROOT / \"X_windows.npy\")\n",
    "y_all = np.load(PROCESSED_ROOT / \"y_windows.npy\")\n",
    "\n",
    "# Load class mappings\n",
    "with open(PROCESSED_ROOT / \"class_mappings.json\", 'r') as f:\n",
    "    class_mappings = json.load(f)\n",
    "\n",
    "classes = class_mappings['classes']\n",
    "class_to_id = class_mappings['class_to_id']\n",
    "id_to_class = class_mappings['id_to_class']\n",
    "\n",
    "with open(PROCESSED_ROOT / \"file_info.pkl\", 'rb') as f:\n",
    "    file_info = pickle.load(f)\n",
    "\n",
    "print(\"Dataset loaded:\")\n",
    "print(f\"  X shape: {X_all.shape}\")\n",
    "print(f\"  y shape: {y_all.shape}\")\n",
    "print(f\"  Classes: {classes}\")\n",
    "print(f\"  Class distribution:\")\n",
    "class_counts = np.bincount(y_all, minlength=len(classes))\n",
    "for i, (cls, count) in enumerate(zip(classes, class_counts)):\n",
    "    ratio = count / len(y_all)\n",
    "    print(f\"    {cls}: {count:,} ({ratio:.3f})\")\n",
    "print(f\"  Files processed: {len(file_info)}\")\n",
    "print(f\"  Total windows: {len(X_all)}\")\n",
    "print(f\"  Features per timestep: {X_all.shape[2]}\")\n",
    "\n",
    "# Create groups for GroupShuffleSplit (one group per video)\n",
    "groups = create_groups_from_metadata(file_info, y_all)\n",
    "unique_videos = len(np.unique(groups))\n",
    "print(f\"  Unique video groups: {unique_videos}\")\n",
    "\n",
    "# Prepare labels for two-stage approach\n",
    "none_id = class_to_id[\"none\"]\n",
    "action_classes = [cls for cls in classes if cls != \"none\"]\n",
    "\n",
    "y_binary = prepare_binary_labels(y_all, none_id)\n",
    "action_mask, y_action = prepare_action_labels(y_all, none_id, action_classes, class_to_id)\n",
    "\n",
    "print(f\"\\nTwo-stage setup:\")\n",
    "print(f\"  Stage A (binary): {y_binary.sum()}/{len(y_binary)} action windows ({y_binary.mean():.3f})\")\n",
    "print(f\"  Stage B (4-class): {len(y_action)} action windows for classification\")\n",
    "print(f\"  Action classes: {action_classes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65171e5d",
   "metadata": {},
   "source": [
    "## Two-Stage Hierarchical Classification\n",
    "\n",
    "Implement proper evaluation for extreme class imbalance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee5ba93",
   "metadata": {},
   "source": [
    "## Two-Stage Hierarchical Classification\n",
    "\n",
    "Implement proper evaluation for extreme class imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1f16d8",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1065872545.py, line 171)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[19], line 171\u001b[1;36m\u001b[0m\n\u001b[1;33m    <VSCode.Cell id=\"#VSC-bd1a0e16\" language=\"markdown\">\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Reservoir parameters\n",
    "RESERVOIR_PARAMS = {\n",
    "    'units': 300,\n",
    "    'sr': 0.9,\n",
    "    'lr': 0.3,\n",
    "    'input_scaling': 0.5,\n",
    "    'seed': 42\n",
    "}\n",
    "\n",
    "# Build reservoir\n",
    "reservoir = build_reservoir(**RESERVOIR_PARAMS)\n",
    "print(f\"Reservoir built: {RESERVOIR_PARAMS}\")\n",
    "\n",
    "# Prepare features\n",
    "print(\"\\nPreparing features...\")\n",
    "\n",
    "# Baseline features (flattened + standardized)\n",
    "X_flat = X_all.reshape(X_all.shape[0], -1)\n",
    "scaler_flat = StandardScaler()\n",
    "X_flat_scaled = scaler_flat.fit_transform(X_flat)\n",
    "\n",
    "# Reservoir features (last state pooling)\n",
    "print(\"Computing reservoir features (last state pooling)...\")\n",
    "X_reservoir_last = reservoir_transform(reservoir, X_all, pooling=\"last\")\n",
    "scaler_reservoir = StandardScaler()\n",
    "X_reservoir_last_scaled = scaler_reservoir.fit_transform(X_reservoir_last)\n",
    "\n",
    "# Reservoir features (mean pooling)\n",
    "reservoir.reset()  # Reset for second pass\n",
    "print(\"Computing reservoir features (mean pooling)...\")\n",
    "X_reservoir_mean = reservoir_transform(reservoir, X_all, pooling=\"mean\")\n",
    "X_reservoir_mean_scaled = scaler_reservoir.fit_transform(X_reservoir_mean)\n",
    "\n",
    "print(\"Features prepared:\")\n",
    "print(f\"  Baseline: {X_flat_scaled.shape}\")\n",
    "print(f\"  Reservoir (last): {X_reservoir_last_scaled.shape}\")\n",
    "print(f\"  Reservoir (mean): {X_reservoir_mean_scaled.shape}\")\n",
    "\n",
    "# Define feature sets for comparison\n",
    "feature_sets = {\n",
    "    'Baseline': X_flat_scaled,\n",
    "    'Reservoir_Last': X_reservoir_last_scaled,\n",
    "    'Reservoir_Mean': X_reservoir_mean_scaled\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TWO-STAGE HIERARCHICAL CLASSIFICATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Stage A: Action vs None Detection\n",
    "print(\"\\n\" + \"-\"*40)\n",
    "print(\"STAGE A: Action vs None Detection\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "stageA_results = {}\n",
    "for feature_name, features in feature_sets.items():\n",
    "    print(f\"\\nTraining {feature_name}...\")\n",
    "    results = train_and_eval_stageA(features, y_binary, groups, feature_name)\n",
    "    stageA_results[feature_name] = results\n",
    "    \n",
    "    print(f\"  PR-AUC: {results['pr_auc']:.3f}\")\n",
    "    print(f\"  Best threshold: {results['best_threshold']:.3f}\")\n",
    "    print(f\"  Precision: {results['precision']:.3f}\")\n",
    "    print(f\"  Recall: {results['recall']:.3f}\")\n",
    "    print(f\"  F1: {results['f1']:.3f}\")\n",
    "    print(f\"  F-beta (β=2): {results['fbeta']:.3f}\")\n",
    "\n",
    "# Stage B: Multi-class Action Classification\n",
    "print(\"\\n\" + \"-\"*40)\n",
    "print(\"STAGE B: Multi-class Action Classification\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "stageB_results = {}\n",
    "for feature_name, features in feature_sets.items():\n",
    "    print(f\"\\nTraining {feature_name}...\")\n",
    "    results = train_and_eval_stageB(features, y_action, groups, action_mask, feature_name, action_classes)\n",
    "    stageB_results[feature_name] = results\n",
    "    \n",
    "    print(f\"  Macro F1: {results['macro_f1']:.3f}\")\n",
    "    print(\"  Per-class F1:\")\n",
    "    for cls in action_classes:\n",
    "        f1 = results['classification_report'][cls]['f1-score']\n",
    "        print(f\"    {cls}: {f1:.3f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SUMMARY AND VISUALIZATIONS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Summary table\n",
    "print(\"\\nSTAGE A RESULTS (Action Detection):\")\n",
    "print(\"Feature Set      | PR-AUC | Threshold | Precision | Recall | F1    | F-β(2)\")\n",
    "print(\"-\" * 75)\n",
    "for name, results in stageA_results.items():\n",
    "    print(f\"{name:<15} | {results['pr_auc']:<6.3f} | {results['best_threshold']:<9.3f} | {results['precision']:<9.3f} | {results['recall']:<6.3f} | {results['f1']:<5.3f} | {results['fbeta']:.3f}\")\n",
    "\n",
    "print(\"\\nSTAGE B RESULTS (Action Classification):\")\n",
    "print(\"Feature Set      | Macro F1 | Chase | Avoid | Attack | ChaseAttack\")\n",
    "print(\"-\" * 70)\n",
    "for name, results in stageB_results.items():\n",
    "    f1_scores = [results['classification_report'][cls]['f1-score'] for cls in action_classes]\n",
    "    f1_str = \" | \".join(f\"{f1:.3f}\" for f1 in f1_scores)\n",
    "    print(f\"{name:<15} | {results['macro_f1']:<8.3f} | {f1_str}\")\n",
    "\n",
    "# Visualizations\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Stage A: Confusion matrices\n",
    "for i, (name, results) in enumerate(stageA_results.items()):\n",
    "    cm = results['confusion_matrix']\n",
    "    axes[i].imshow(cm, cmap='Blues')\n",
    "    axes[i].set_title(f'Stage A: {name}')\n",
    "    axes[i].set_xlabel('Predicted')\n",
    "    axes[i].set_ylabel('True')\n",
    "    axes[i].set_xticks([0, 1])\n",
    "    axes[i].set_yticks([0, 1])\n",
    "    axes[i].set_xticklabels(['None', 'Action'])\n",
    "    axes[i].set_yticklabels(['None', 'Action'])\n",
    "    \n",
    "    # Add text\n",
    "    for x in range(2):\n",
    "        for y in range(2):\n",
    "            axes[i].text(y, x, cm[x, y], ha='center', va='center')\n",
    "\n",
    "# Stage B: Confusion matrices\n",
    "for i, (name, results) in enumerate(stageB_results.items()):\n",
    "    cm = results['confusion_matrix']\n",
    "    axes[i+3].imshow(cm, cmap='Blues')\n",
    "    axes[i+3].set_title(f'Stage B: {name}')\n",
    "    axes[i+3].set_xlabel('Predicted')\n",
    "    axes[i+3].set_ylabel('True')\n",
    "    axes[i+3].set_xticks(range(len(action_classes)))\n",
    "    axes[i+3].set_yticks(range(len(action_classes)))\n",
    "    axes[i+3].set_xticklabels(action_classes, rotation=45, ha='right')\n",
    "    axes[i+3].set_yticklabels(action_classes)\n",
    "    \n",
    "    # Add text\n",
    "    for x in range(len(action_classes)):\n",
    "        for y in range(len(action_classes)):\n",
    "            axes[i+3].text(y, x, cm[x, y], ha='center', va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Threshold sweep for best Stage A model\n",
    "best_stageA = max(stageA_results.items(), key=lambda x: x[1]['fbeta'])\n",
    "best_name, best_results = best_stageA\n",
    "\n",
    "print(f\"\\nTHRESHOLD SWEEP for {best_name} (Stage A):\")\n",
    "thresholds = best_results['threshold_sweep']['thresholds']\n",
    "fbeta_scores = best_results['threshold_sweep']['fbeta_scores']\n",
    "\n",
    "# Find top 5 thresholds\n",
    "top_indices = np.argsort(fbeta_scores)[-5:][::-1]\n",
    "print(\"Top 5 thresholds by F-β(2) score:\")\n",
    "for i, idx in enumerate(top_indices):\n",
    "    thresh = thresholds[idx]\n",
    "    fbeta = fbeta_scores[idx]\n",
    "    print(f\"  {i+1}. Threshold {thresh:.3f}: F-β(2) = {fbeta:.3f}\")\n",
    "\n",
    "# Action detection statistics\n",
    "total_action_windows = y_binary.sum()\n",
    "detected_by_best = (best_results['y_val_prob'] >= best_results['best_threshold']).sum()\n",
    "print(f\"\\nACTION DETECTION STATISTICS:\")\n",
    "print(f\"  Total action windows in validation: {total_action_windows}\")\n",
    "print(f\"  Detected by {best_name}: {detected_by_best}\")\n",
    "print(f\"  Detection rate: {detected_by_best/total_action_windows:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

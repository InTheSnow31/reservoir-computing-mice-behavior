{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07fb3a22",
   "metadata": {},
   "source": [
    "# MABe – Dataset processing (windowing + labels + metadata)\n",
    "\n",
    "Goal:\n",
    "Build a reusable processed dataset for training:\n",
    "\n",
    "Outputs saved to `data/data_processed/`:\n",
    "- `X_windows.npy`                     (N, T, D)\n",
    "- `y_windows.npy`                     (N,)\n",
    "- `video_id_windows.npy`              (N,)\n",
    "- `category_windows.npy`              (N,)\n",
    "- `mouse_id_windows.npy`              (N,)\n",
    "- `window_start_frame_windows.npy`    (N,)\n",
    "- `class_mappings.json`               (label ↔ id)\n",
    "- `file_info.pkl`                     (per-file processing summary)\n",
    "\n",
    "We will proceed in steps:\n",
    "1) Setup and paths\n",
    "2) Define label mapping and windowing rule\n",
    "3) Identify common bodyparts (stable feature set)\n",
    "4) Process each tracking file (X) and annotation file (segments)\n",
    "5) Build window-level labels (multi-class)\n",
    "6) Save arrays and metadata\n",
    "7) Sanity checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa7c8f16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyarrow in c:\\users\\flore\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (22.0.0)\n",
      "Requirement already satisfied: fastparquet in c:\\users\\flore\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2025.12.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\flore\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.2.6)\n",
      "Requirement already satisfied: pandas in c:\\users\\flore\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.3.3)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\flore\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.7.2)\n",
      "Requirement already satisfied: reservoirpy in c:\\users\\flore\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.4.1)\n",
      "Requirement already satisfied: cramjam>=2.3 in c:\\users\\flore\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from fastparquet) (2.11.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\flore\\appdata\\roaming\\python\\python310\\site-packages (from fastparquet) (25.0)\n",
      "Requirement already satisfied: fsspec in c:\\users\\flore\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from fastparquet) (2026.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\flore\\appdata\\roaming\\python\\python310\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\flore\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (2025.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\flore\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\flore\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\flore\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn) (1.5.3)\n",
      "Requirement already satisfied: scipy>=1.8.0 in c:\\users\\flore\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn) (1.15.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\flore\\appdata\\roaming\\python\\python310\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install pyarrow fastparquet numpy pandas scikit-learn reservoirpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb53707c",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "We import libraries for:\n",
    "- file system navigation (Path)\n",
    "- parquet loading (pandas + pyarrow)\n",
    "- numeric arrays (numpy)\n",
    "- saving metadata (json, pickle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f99f7581",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3eea740",
   "metadata": {},
   "source": [
    "## Data paths\n",
    "\n",
    "We use the structure:\n",
    "- raw tracking: `data/data_raw/train_tracking/<category>/<video_id>.parquet`\n",
    "- raw annotation: `data/data_raw/train_annotation/<category>/<video_id>.parquet`\n",
    "- processed output: `data/data_processed/`\n",
    "\n",
    "Cell objective: create `data/data_processed/` if needed and print paths for verification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d0ea53b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRACK_ROOT exists: True C:\\perso\\Ensc\\3A\\IA\\reservoir\\reservoir-computing-mice-behavior\\data\\data_raw\\train_tracking\n",
      "ANNOT_ROOT exists: True C:\\perso\\Ensc\\3A\\IA\\reservoir\\reservoir-computing-mice-behavior\\data\\data_raw\\train_annotation\n",
      "PROCESSED_ROOT: C:\\perso\\Ensc\\3A\\IA\\reservoir\\reservoir-computing-mice-behavior\\data\\data_processed\n"
     ]
    }
   ],
   "source": [
    "TRACK_ROOT = Path(\"data/data_raw/train_tracking\")\n",
    "ANNOT_ROOT = Path(\"data/data_raw/train_annotation\")\n",
    "PROCESSED_ROOT = Path(\"data/data_processed\")\n",
    "PROCESSED_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"TRACK_ROOT exists:\", TRACK_ROOT.exists(), TRACK_ROOT.resolve())\n",
    "print(\"ANNOT_ROOT exists:\", ANNOT_ROOT.exists(), ANNOT_ROOT.resolve())\n",
    "print(\"PROCESSED_ROOT:\", PROCESSED_ROOT.resolve())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8ed1d1",
   "metadata": {},
   "source": [
    "## Global configuration\n",
    "\n",
    "We decide:\n",
    "- which mouse_id we process (start simple with one mouse)\n",
    "- window size and step\n",
    "- which categories/files to process (debug subset first, then all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6bf9750d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found categories: ['AdaptableSnail', 'BoisterousParrot', 'CRIM13', 'CalMS21_supplemental', 'CalMS21_task1', 'CalMS21_task2', 'CautiousGiraffe', 'DeliriousFly', 'ElegantMink', 'GroovyShrew', 'InvincibleJellyfish', 'JovialSwallow', 'LyricalHare', 'MABe22_keypoints', 'MABe22_movies', 'NiftyGoldfinch', 'PleasantMeerkat', 'ReflectiveManatee', 'SparklingTapir', 'TranquilPanther', 'UppityFerret']\n"
     ]
    }
   ],
   "source": [
    "# Minimal, reproducible defaults\n",
    "MOUSE_ID = 1\n",
    "\n",
    "WINDOW_SIZE = 200\n",
    "STEP = 200\n",
    "\n",
    "# Debug mode: process only a few files per category first\n",
    "DEBUG = True\n",
    "MAX_FILES_PER_CATEGORY = 5  # set None for all files\n",
    "\n",
    "# Categories to process\n",
    "categories = sorted([d.name for d in TRACK_ROOT.iterdir() if d.is_dir()])\n",
    "print(\"Found categories:\", categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73923533",
   "metadata": {},
   "source": [
    "## Label mapping and multi-class rule\n",
    "\n",
    "Annotations are segments:\n",
    "(agent_id, target_id, action, start_frame, stop_frame)\n",
    "\n",
    "We will build a window label by looking at which actions overlap the window.\n",
    "If multiple actions overlap, we resolve conflicts using a priority order.\n",
    "\n",
    "Next cell objective: define the class mapping (including \"none\") and a priority order.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "afda7cee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class_to_id: {'none': 0, 'chase': 1, 'avoid': 2, 'attack': 3, 'chaseattack': 4}\n",
      "priority_order: ['attack', 'chaseattack', 'chase', 'avoid', 'none']\n"
     ]
    }
   ],
   "source": [
    "# Define label vocabulary\n",
    "# \"none\" means no annotated action overlaps the window\n",
    "classes = [\"none\", \"chase\", \"avoid\", \"attack\", \"chaseattack\"]\n",
    "\n",
    "class_to_id = {c: i for i, c in enumerate(classes)}\n",
    "id_to_class = {i: c for c, i in class_to_id.items()}\n",
    "\n",
    "# Priority order when multiple actions overlap a window\n",
    "# Put the most specific / important actions first if you want\n",
    "priority_order = [\"attack\", \"chaseattack\", \"chase\", \"avoid\", \"none\"]\n",
    "\n",
    "print(\"class_to_id:\", class_to_id)\n",
    "print(\"priority_order:\", priority_order)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5887be",
   "metadata": {},
   "source": [
    "## Helper functions (I/O)\n",
    "\n",
    "We define:\n",
    "- `load_parquet_df(path)` for robustness\n",
    "- `load_annotation(video_id, category)` to load the matching annotation file\n",
    "\n",
    "Next cell objective: implement these functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d33eceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_parquet_df(path: Path) -> pd.DataFrame:\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(str(path))\n",
    "    return pd.read_parquet(path)\n",
    "\n",
    "def load_annotation(category: str, video_id: str) -> pd.DataFrame:\n",
    "    annot_path = ANNOT_ROOT / category / f\"{video_id}.parquet\"\n",
    "    if not annot_path.exists():\n",
    "        # Some datasets might have missing annotations, handle gracefully upstream\n",
    "        return pd.DataFrame(columns=[\"agent_id\", \"target_id\", \"action\", \"start_frame\", \"stop_frame\"])\n",
    "    return pd.read_parquet(annot_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3d5d73",
   "metadata": {},
   "source": [
    "## Stable feature set (common bodyparts)\n",
    "\n",
    "Tracking files can differ in which bodyparts are present.\n",
    "To build a consistent feature matrix, we compute the intersection of bodyparts across selected files.\n",
    "\n",
    "Next cell objective: implement `find_common_bodyparts` and run it on a subset of files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec1168bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Common bodyparts count: 3\n",
      "Common bodyparts: ['ear_left', 'ear_right', 'tail_base']\n"
     ]
    }
   ],
   "source": [
    "def find_common_bodyparts(categories, max_files_per_category=5):\n",
    "    common = None\n",
    "    \n",
    "    for category in categories:\n",
    "        files = sorted((TRACK_ROOT / category).glob(\"*.parquet\"))\n",
    "        if max_files_per_category is not None:\n",
    "            files = files[:max_files_per_category]\n",
    "        \n",
    "        for f in files:\n",
    "            df = pd.read_parquet(f, columns=[\"bodypart\"])\n",
    "            bps = set(df[\"bodypart\"].unique())\n",
    "            common = bps if common is None else (common & bps)\n",
    "    \n",
    "    return sorted(list(common)) if common is not None else []\n",
    "\n",
    "common_bodyparts = find_common_bodyparts(\n",
    "    categories=categories,\n",
    "    max_files_per_category=(MAX_FILES_PER_CATEGORY if DEBUG else 10)\n",
    ")\n",
    "\n",
    "print(\"Common bodyparts count:\", len(common_bodyparts))\n",
    "print(\"Common bodyparts:\", common_bodyparts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd77d723",
   "metadata": {},
   "source": [
    "## Build X(t) from tracking (single file)\n",
    "\n",
    "We convert the long tracking format into a wide matrix:\n",
    "- index = video_frame\n",
    "- columns = x_<bodypart>, y_<bodypart>\n",
    "\n",
    "Then we:\n",
    "- keep only `common_bodyparts` (stable features)\n",
    "- interpolate and fill remaining missing values\n",
    "- return:\n",
    "  - X_final: NumPy array (T, D)\n",
    "  - frames: NumPy array (T,) with frame indices\n",
    "\n",
    "Next cell objective: implement `build_X_for_file(...)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c2e66866",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_X_for_file(tracking_df: pd.DataFrame, mouse_id: int, common_bodyparts: list[str]) -> tuple[np.ndarray, np.ndarray, list[str]]:\n",
    "    # Filter one mouse\n",
    "    df_mouse = tracking_df[tracking_df[\"mouse_id\"] == mouse_id].copy()\n",
    "    if df_mouse.empty:\n",
    "        return np.zeros((0, 0), dtype=np.float32), np.zeros((0,), dtype=np.int64), []\n",
    "\n",
    "    # Pivot to wide\n",
    "    df_wide = (\n",
    "        df_mouse\n",
    "        .pivot(index=\"video_frame\", columns=\"bodypart\", values=[\"x\", \"y\"])\n",
    "        .sort_index()\n",
    "    )\n",
    "\n",
    "    # Flatten columns (MultiIndex -> strings)\n",
    "    df_wide.columns = [f\"{coord}_{part}\" for coord, part in df_wide.columns]\n",
    "\n",
    "    # Keep only common bodyparts\n",
    "    cols_keep = []\n",
    "    for bp in common_bodyparts:\n",
    "        cx = f\"x_{bp}\"\n",
    "        cy = f\"y_{bp}\"\n",
    "        if cx in df_wide.columns and cy in df_wide.columns:\n",
    "            cols_keep.extend([cx, cy])\n",
    "\n",
    "    df_feat = df_wide[cols_keep].copy()\n",
    "\n",
    "    # Interpolate time-wise and fill edges\n",
    "    df_feat = df_feat.interpolate(method=\"linear\").ffill().bfill()\n",
    "\n",
    "    X_final = df_feat.to_numpy(dtype=np.float32)\n",
    "    frames = df_feat.index.to_numpy(dtype=np.int64)\n",
    "\n",
    "    return X_final, frames, cols_keep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "285ccf1b",
   "metadata": {},
   "source": [
    "## Build window labels from segment annotations (multi-class)\n",
    "\n",
    "We label each window [start_frame, end_frame] based on segment overlap:\n",
    "- filter segments where agent_id == MOUSE_ID\n",
    "- check overlap with the window\n",
    "- if no overlap => \"none\"\n",
    "- if overlap with multiple actions => pick using `priority_order`\n",
    "\n",
    "Next cell objective: implement a function that converts segments into window labels and returns y_windows + window start frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e623e1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_window_from_segments(ann: pd.DataFrame, mouse_id: int, w_start: int, w_end: int,\n",
    "                               class_to_id: dict, priority_order: list[str]) -> int:\n",
    "    \"\"\"\n",
    "    Return class id for one window.\n",
    "    Overlap definition: segment overlaps if stop_frame >= w_start and start_frame <= w_end\n",
    "    \"\"\"\n",
    "    if ann.empty:\n",
    "        return class_to_id[\"none\"]\n",
    "\n",
    "    ann_sel = ann[ann[\"agent_id\"] == mouse_id]\n",
    "    if ann_sel.empty:\n",
    "        return class_to_id[\"none\"]\n",
    "\n",
    "    overlaps = ann_sel[(ann_sel[\"stop_frame\"] >= w_start) & (ann_sel[\"start_frame\"] <= w_end)]\n",
    "    if overlaps.empty:\n",
    "        return class_to_id[\"none\"]\n",
    "\n",
    "    actions = set(overlaps[\"action\"].astype(str).unique())\n",
    "\n",
    "    for a in priority_order:\n",
    "        if a in actions:\n",
    "            return class_to_id[a]\n",
    "\n",
    "    return class_to_id[\"none\"]\n",
    "\n",
    "def make_windows_multiclass(X: np.ndarray, frames: np.ndarray, ann: pd.DataFrame, mouse_id: int,\n",
    "                            window_size: int, step: int,\n",
    "                            class_to_id: dict, priority_order: list[str]):\n",
    "    \"\"\"\n",
    "    Build:\n",
    "    - X_windows: (N, window_size, D)\n",
    "    - y_windows: (N,)\n",
    "    - window_start_frames: (N,)\n",
    "    \"\"\"\n",
    "    T = X.shape[0]\n",
    "    if T < window_size:\n",
    "        return (\n",
    "            np.zeros((0, window_size, X.shape[1]), dtype=np.float32),\n",
    "            np.zeros((0,), dtype=np.int64),\n",
    "            np.zeros((0,), dtype=np.int64),\n",
    "        )\n",
    "\n",
    "    Xw = []\n",
    "    yw = []\n",
    "    w_starts = []\n",
    "\n",
    "    for i0 in range(0, T - window_size + 1, step):\n",
    "        i1 = i0 + window_size - 1\n",
    "        w_start = int(frames[i0])\n",
    "        w_end = int(frames[i1])\n",
    "\n",
    "        y_id = label_window_from_segments(ann, mouse_id, w_start, w_end, class_to_id, priority_order)\n",
    "\n",
    "        Xw.append(X[i0:i0 + window_size])\n",
    "        yw.append(y_id)\n",
    "        w_starts.append(w_start)\n",
    "\n",
    "    return (\n",
    "        np.stack(Xw).astype(np.float32),\n",
    "        np.array(yw, dtype=np.int64),\n",
    "        np.array(w_starts, dtype=np.int64),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bddc1f6",
   "metadata": {},
   "source": [
    "## Process all files\n",
    "\n",
    "For each category and each tracking file:\n",
    "1) Load tracking\n",
    "2) Load the matching annotation file (same video_id)\n",
    "3) Build X(t) for one mouse\n",
    "4) Build window dataset + window labels\n",
    "5) Append to global arrays\n",
    "6) Record per-file summary in `file_info`\n",
    "\n",
    "Next cell objective: implement the processing loop (with debug limits) and store metadata arrays aligned with windows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8336be79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing category: AdaptableSnail (5 files)\n",
      "  1212811043: windows=367  shape=(367, 200, 6)\n",
      "  1260392287: windows=269  shape=(269, 200, 6)\n",
      "  1351098077: windows=400  shape=(400, 200, 6)\n",
      "  1408652858: windows=92  shape=(92, 200, 6)\n",
      "  143861384: windows=417  shape=(417, 200, 6)\n",
      "\n",
      "Processing category: BoisterousParrot (5 files)\n",
      "  1059582964: windows=2970  shape=(2970, 200, 6)\n",
      "  1184291605: windows=2970  shape=(2970, 200, 6)\n",
      "  1201849558: windows=2970  shape=(2970, 200, 6)\n",
      "  1459695188: windows=2970  shape=(2970, 200, 6)\n",
      "  1985626297: windows=2970  shape=(2970, 200, 6)\n",
      "\n",
      "Processing category: CRIM13 (5 files)\n",
      "  1009459450: windows=55  shape=(55, 200, 6)\n",
      "  1057221056: windows=41  shape=(41, 200, 6)\n",
      "  1149348188: windows=40  shape=(40, 200, 6)\n",
      "  1213233769: windows=73  shape=(73, 200, 6)\n",
      "  1313797424: windows=44  shape=(44, 200, 6)\n",
      "\n",
      "Processing category: CalMS21_supplemental (5 files)\n",
      "  1006083669: windows=91  shape=(91, 200, 6)\n",
      "  1012566686: windows=133  shape=(133, 200, 6)\n",
      "  1012566850: windows=97  shape=(97, 200, 6)\n",
      "  1013428301: windows=52  shape=(52, 200, 6)\n",
      "  1014350333: windows=32  shape=(32, 200, 6)\n",
      "\n",
      "Processing category: CalMS21_task1 (5 files)\n",
      "  1011533560: windows=30  shape=(30, 200, 6)\n",
      "  1036913557: windows=59  shape=(59, 200, 6)\n",
      "  1062887364: windows=28  shape=(28, 200, 6)\n",
      "  1087101060: windows=22  shape=(22, 200, 6)\n",
      "  1089428234: windows=86  shape=(86, 200, 6)\n",
      "\n",
      "Processing category: CalMS21_task2 (5 files)\n",
      "  1025257155: windows=33  shape=(33, 200, 6)\n",
      "  1071207417: windows=88  shape=(88, 200, 6)\n",
      "  1095059853: windows=93  shape=(93, 200, 6)\n",
      "  1121857128: windows=89  shape=(89, 200, 6)\n",
      "  1155324226: windows=137  shape=(137, 200, 6)\n",
      "\n",
      "Processing category: CautiousGiraffe (5 files)\n",
      "  1341883680: windows=75  shape=(75, 200, 6)\n",
      "  1539773935: windows=74  shape=(74, 200, 6)\n",
      "  1657896715: windows=74  shape=(74, 200, 6)\n",
      "  1729143180: windows=74  shape=(74, 200, 6)\n",
      "  21954203: windows=73  shape=(73, 200, 6)\n",
      "\n",
      "Processing category: DeliriousFly (5 files)\n",
      "  1378394926: windows=270  shape=(270, 200, 6)\n",
      "  1549344783: windows=269  shape=(269, 200, 6)\n",
      "  1649549863: windows=269  shape=(269, 200, 6)\n",
      "  1894091332: windows=270  shape=(270, 200, 6)\n",
      "  246051591: windows=269  shape=(269, 200, 6)\n",
      "\n",
      "Processing category: ElegantMink (5 files)\n",
      "  1045757672: windows=132  shape=(132, 200, 6)\n",
      "  1176870705: windows=264  shape=(264, 200, 6)\n",
      "  1430254576: windows=267  shape=(267, 200, 6)\n",
      "  1504809717: windows=260  shape=(260, 200, 6)\n",
      "  1512066687: windows=269  shape=(269, 200, 6)\n",
      "\n",
      "Processing category: GroovyShrew (5 files)\n",
      "  1057262087: windows=107  shape=(107, 200, 6)\n",
      "  1118505887: windows=112  shape=(112, 200, 6)\n",
      "  1194163628: windows=107  shape=(107, 200, 6)\n",
      "  1205169652: windows=173  shape=(173, 200, 6)\n",
      "  1239537257: windows=112  shape=(112, 200, 6)\n",
      "\n",
      "Processing category: InvincibleJellyfish (5 files)\n",
      "  1106234543: windows=85  shape=(85, 200, 6)\n",
      "  1108562754: windows=85  shape=(85, 200, 6)\n",
      "  1171931756: windows=85  shape=(85, 200, 6)\n",
      "  1205744881: windows=85  shape=(85, 200, 6)\n",
      "  1239915365: windows=85  shape=(85, 200, 6)\n",
      "\n",
      "Processing category: JovialSwallow (5 files)\n",
      "  1096442159: windows=44  shape=(44, 200, 6)\n",
      "  1098020586: windows=44  shape=(44, 200, 6)\n",
      "  1164701770: windows=44  shape=(44, 200, 6)\n",
      "  1212894105: windows=44  shape=(44, 200, 6)\n",
      "  1231275297: windows=44  shape=(44, 200, 6)\n",
      "\n",
      "Processing category: LyricalHare (5 files)\n",
      "  1023347338: windows=183  shape=(183, 200, 6)\n",
      "  1117939052: windows=183  shape=(183, 200, 6)\n",
      "  121552177: windows=180  shape=(180, 200, 6)\n",
      "  1254387004: windows=182  shape=(182, 200, 6)\n",
      "  1300539709: windows=180  shape=(180, 200, 6)\n",
      "\n",
      "Processing category: MABe22_keypoints (5 files)\n",
      "  1000217804: windows=9  shape=(9, 200, 6)\n",
      "  1000285113: windows=9  shape=(9, 200, 6)\n",
      "  1001006733: windows=9  shape=(9, 200, 6)\n",
      "  1001081757: windows=9  shape=(9, 200, 6)\n",
      "  1001941944: windows=9  shape=(9, 200, 6)\n",
      "\n",
      "Processing category: MABe22_movies (5 files)\n",
      "  1000942438: windows=9  shape=(9, 200, 6)\n",
      "  1001881563: windows=9  shape=(9, 200, 6)\n",
      "  1003032749: windows=9  shape=(9, 200, 6)\n",
      "  1004489648: windows=9  shape=(9, 200, 6)\n",
      "  1004806647: windows=9  shape=(9, 200, 6)\n",
      "\n",
      "Processing category: NiftyGoldfinch (5 files)\n",
      "  101686631: windows=41  shape=(41, 200, 6)\n",
      "  1223664597: windows=176  shape=(176, 200, 6)\n",
      "  1233426234: windows=176  shape=(176, 200, 6)\n",
      "  1269061587: windows=137  shape=(137, 200, 6)\n",
      "  1539947238: windows=177  shape=(177, 200, 6)\n",
      "\n",
      "Processing category: PleasantMeerkat (5 files)\n",
      "  1089480847: windows=134  shape=(134, 200, 6)\n",
      "  1133952127: windows=133  shape=(133, 200, 6)\n",
      "  1177059156: windows=132  shape=(132, 200, 6)\n",
      "  1212669906: windows=135  shape=(135, 200, 6)\n",
      "  1266446081: windows=133  shape=(133, 200, 6)\n",
      "\n",
      "Processing category: ReflectiveManatee (5 files)\n",
      "  1270744933: windows=75  shape=(75, 200, 6)\n",
      "  1458170545: windows=76  shape=(76, 200, 6)\n",
      "  1663417188: windows=75  shape=(75, 200, 6)\n",
      "  1728439177: windows=74  shape=(74, 200, 6)\n",
      "  1859225348: windows=76  shape=(76, 200, 6)\n",
      "\n",
      "Processing category: SparklingTapir (5 files)\n",
      "  1063876199: windows=8  shape=(8, 200, 6)\n",
      "  1085312517: windows=20  shape=(20, 200, 6)\n",
      "  1086931654: windows=90  shape=(90, 200, 6)\n",
      "  1105416294: windows=61  shape=(61, 200, 6)\n",
      "  1127527475: windows=91  shape=(91, 200, 6)\n",
      "\n",
      "Processing category: TranquilPanther (5 files)\n",
      "  1057775501: windows=83  shape=(83, 200, 6)\n",
      "  1360332361: windows=82  shape=(82, 200, 6)\n",
      "  1456719978: windows=85  shape=(85, 200, 6)\n",
      "  1462669248: windows=715  shape=(715, 200, 6)\n",
      "  1510465149: windows=262  shape=(262, 200, 6)\n",
      "\n",
      "Processing category: UppityFerret (5 files)\n",
      "  1085105007: windows=73  shape=(73, 200, 6)\n",
      "  1131517877: windows=75  shape=(75, 200, 6)\n",
      "  1170955420: windows=74  shape=(74, 200, 6)\n",
      "  1197493895: windows=74  shape=(74, 200, 6)\n",
      "  1241990995: windows=74  shape=(74, 200, 6)\n"
     ]
    }
   ],
   "source": [
    "all_X = []\n",
    "all_y = []\n",
    "all_video_id = []\n",
    "all_category = []\n",
    "all_mouse_id = []\n",
    "all_w_start = []\n",
    "\n",
    "file_info = []\n",
    "\n",
    "for category in categories:\n",
    "    category_path = TRACK_ROOT / category\n",
    "    tracking_files = sorted(category_path.glob(\"*.parquet\"))\n",
    "    if DEBUG and MAX_FILES_PER_CATEGORY is not None:\n",
    "        tracking_files = tracking_files[:MAX_FILES_PER_CATEGORY]\n",
    "\n",
    "    print(f\"\\nProcessing category: {category} ({len(tracking_files)} files)\")\n",
    "\n",
    "    for tracking_path in tracking_files:\n",
    "        video_id = tracking_path.stem\n",
    "\n",
    "        # Load raw data\n",
    "        tracking_df = load_parquet_df(tracking_path)\n",
    "        ann = load_annotation(category, video_id)\n",
    "\n",
    "        # Build X(t)\n",
    "        X_final, frames, cols_keep = build_X_for_file(tracking_df, MOUSE_ID, common_bodyparts)\n",
    "\n",
    "        if X_final.shape[0] == 0:\n",
    "            file_info.append({\n",
    "                \"category\": category,\n",
    "                \"video_id\": video_id,\n",
    "                \"status\": \"skip_empty_mouse\",\n",
    "                \"n_windows\": 0\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        # Windowing + labels\n",
    "        Xw, yw, w_starts = make_windows_multiclass(\n",
    "            X=X_final,\n",
    "            frames=frames,\n",
    "            ann=ann,\n",
    "            mouse_id=MOUSE_ID,\n",
    "            window_size=WINDOW_SIZE,\n",
    "            step=STEP,\n",
    "            class_to_id=class_to_id,\n",
    "            priority_order=priority_order\n",
    "        )\n",
    "\n",
    "        if Xw.shape[0] == 0:\n",
    "            file_info.append({\n",
    "                \"category\": category,\n",
    "                \"video_id\": video_id,\n",
    "                \"status\": \"skip_too_short\",\n",
    "                \"n_windows\": 0\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        # Append global\n",
    "        all_X.append(Xw)\n",
    "        all_y.append(yw)\n",
    "        all_video_id.append(np.array([video_id] * len(yw), dtype=object))\n",
    "        all_category.append(np.array([category] * len(yw), dtype=object))\n",
    "        all_mouse_id.append(np.array([MOUSE_ID] * len(yw), dtype=np.int8))\n",
    "        all_w_start.append(w_starts)\n",
    "\n",
    "        # Per-file summary\n",
    "        class_counts = np.bincount(yw, minlength=len(classes))\n",
    "        file_info.append({\n",
    "            \"category\": category,\n",
    "            \"video_id\": video_id,\n",
    "            \"status\": \"ok\",\n",
    "            \"n_windows\": int(len(yw)),\n",
    "            \"features\": int(Xw.shape[2]),\n",
    "            \"class_counts\": class_counts.tolist()\n",
    "        })\n",
    "\n",
    "        print(f\"  {video_id}: windows={len(yw)}  shape={Xw.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c173817a",
   "metadata": {},
   "source": [
    "## Concatenate and save processed arrays\n",
    "\n",
    "We concatenate all per-file arrays into one dataset and save everything into `data/data_processed/`.\n",
    "\n",
    "Next cell objective: concatenate arrays, write `.npy` outputs and save mappings and file_info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "993b32dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_all: (26498, 200, 6)\n",
      "y_all: (26498,)\n",
      "video_id_all: (26498,)\n",
      "w_start_all: (26498,)\n",
      "\n",
      "Saved to: C:\\perso\\Ensc\\3A\\IA\\reservoir\\reservoir-computing-mice-behavior\\data\\data_processed\n"
     ]
    }
   ],
   "source": [
    "if len(all_X) == 0:\n",
    "    raise RuntimeError(\"No windows were produced. Check paths, mouse_id, and processing settings.\")\n",
    "\n",
    "X_all = np.concatenate(all_X, axis=0)\n",
    "y_all = np.concatenate(all_y, axis=0)\n",
    "video_id_all = np.concatenate(all_video_id, axis=0)\n",
    "category_all = np.concatenate(all_category, axis=0)\n",
    "mouse_id_all = np.concatenate(all_mouse_id, axis=0)\n",
    "w_start_all = np.concatenate(all_w_start, axis=0)\n",
    "\n",
    "print(\"X_all:\", X_all.shape)\n",
    "print(\"y_all:\", y_all.shape)\n",
    "print(\"video_id_all:\", video_id_all.shape)\n",
    "print(\"w_start_all:\", w_start_all.shape)\n",
    "\n",
    "np.save(PROCESSED_ROOT / \"X_windows.npy\", X_all)\n",
    "np.save(PROCESSED_ROOT / \"y_windows.npy\", y_all)\n",
    "np.save(PROCESSED_ROOT / \"video_id_windows.npy\", video_id_all, allow_pickle=True)\n",
    "np.save(PROCESSED_ROOT / \"category_windows.npy\", category_all, allow_pickle=True)\n",
    "np.save(PROCESSED_ROOT / \"mouse_id_windows.npy\", mouse_id_all)\n",
    "np.save(PROCESSED_ROOT / \"window_start_frame_windows.npy\", w_start_all)\n",
    "\n",
    "with open(PROCESSED_ROOT / \"class_mappings.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(\n",
    "        {\n",
    "            \"classes\": classes,\n",
    "            \"class_to_id\": class_to_id,\n",
    "            \"id_to_class\": id_to_class,\n",
    "            \"priority_order\": priority_order\n",
    "        },\n",
    "        f,\n",
    "        indent=2\n",
    "    )\n",
    "\n",
    "with open(PROCESSED_ROOT / \"file_info.pkl\", \"wb\") as f:\n",
    "    pickle.dump(file_info, f)\n",
    "\n",
    "print(\"\\nSaved to:\", PROCESSED_ROOT.resolve())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496dca14",
   "metadata": {},
   "source": [
    "## Sanity checks\n",
    "\n",
    "We verify:\n",
    "- shapes are consistent\n",
    "- no NaNs in X\n",
    "- class distribution is extremely imbalanced (expected)\n",
    "- metadata arrays align with X/y\n",
    "\n",
    "Next cell objective: run sanity checks and print a compact summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b861c75c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== DATASET SUMMARY ===\n",
      "Total windows: 26498\n",
      "Window shape: (200, 6) (timesteps × features)\n",
      "Class distribution:\n",
      "        none:    25902  (0.97751)\n",
      "       chase:       42  (0.00159)\n",
      "       avoid:       51  (0.00192)\n",
      "      attack:      502  (0.01894)\n",
      "  chaseattack:        1  (0.00004)\n",
      "\n",
      "First 5 samples metadata:\n",
      "1212811043 AdaptableSnail 1 0 chase\n",
      "1212811043 AdaptableSnail 1 200 chase\n",
      "1212811043 AdaptableSnail 1 400 none\n",
      "1212811043 AdaptableSnail 1 600 none\n",
      "1212811043 AdaptableSnail 1 800 chase\n"
     ]
    }
   ],
   "source": [
    "assert X_all.shape[0] == y_all.shape[0] == len(video_id_all) == len(category_all) == len(mouse_id_all) == len(w_start_all)\n",
    "assert X_all.shape[1] == WINDOW_SIZE\n",
    "assert not np.isnan(X_all).any()\n",
    "\n",
    "counts = np.bincount(y_all, minlength=len(classes))\n",
    "total = counts.sum()\n",
    "\n",
    "print(\"\\n=== DATASET SUMMARY ===\")\n",
    "print(\"Total windows:\", int(total))\n",
    "print(\"Window shape:\", X_all.shape[1:], \"(timesteps × features)\")\n",
    "print(\"Class distribution:\")\n",
    "for i, c in enumerate(classes):\n",
    "    print(f\"  {c:>10s}: {counts[i]:>8d}  ({counts[i]/total:.5f})\")\n",
    "\n",
    "print(\"\\nFirst 5 samples metadata:\")\n",
    "for k in range(5):\n",
    "    print(video_id_all[k], category_all[k], int(mouse_id_all[k]), int(w_start_all[k]), id_to_class[int(y_all[k])])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
